{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57f2f48e",
   "metadata": {},
   "source": [
    "# Quick Start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2697c453",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Agent](#agent)\n",
    "  - [Creating an Agent](#creating-an-agent)\n",
    "  - [Running An Agent](#running-an-agent)\n",
    "  - [Session Memory](#session-memory)\n",
    "  - [Function Tools](#function-tools)\n",
    "  - [Agents as Tools](#agents-as-tools)\n",
    "  - [Context (TaskEnvironment)](#context-taskenvironment)\n",
    "  - [Structured Output](#structured-output)\n",
    "  - [Input and Output Guardrails](#input-and-output-guardrails)\n",
    "- [Multi-Agent System (MAS)](#multi-agent-system-mas)\n",
    "  - [Pre-built MAS Workflows](#pre-built-mas-workflows)\n",
    "  - [Orchestrator-Worker MAS](#orchestrator-worker-mas)\n",
    "  - [Planner-Executor MAS](#planner-executor-mas)\n",
    "  - [Bring Your Own MAS Workflows](#bring-your-own-mas-workflows)\n",
    "- [What's Next?](#whats-next)\n",
    "\n",
    "\n",
    "# API KEYS (IMPORTANT!!!)\n",
    "\n",
    "To run this notebook, you'll need valid API keys for the LLM providers you want to use. You can provide them in two ways:\n",
    "\n",
    "1. **Using a `.env` file** (recommended): Create a `.env` file in the same folder as this notebook and add your API keys\n",
    "2. **Direct specification**: Uncomment and set your API keys in the code block below\n",
    "\n",
    "**Note:** This notebook demonstrates various LLM providers (OpenAI, Gemini, DeepSeek, Anthropic). If you only have access to certain providers:\n",
    "- **Skip** examples using unavailable providers, or\n",
    "- **Replace** model names with ones you have access to (e.g., replace `\"openai/gpt-4o-mini\"` with `\"gemini/gemini-2.5-flash\"`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3043a0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key_here\"\n",
    "#os.environ[\"GEMINI_API_KEY\"] = \"your_gemini_api_key_here\"\n",
    "#os.environ[\"DEEPSEEK_API_KEY\"] = \"your_deepseek_api_key_here\"\n",
    "#os.environ[\"ANTHROPIC_API_KEY\"] = \"your_anthropic_api_key_here\"\n",
    "# Additional api keys can be set in a similar manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b73055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case src is not in the path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), \"src\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507e190b",
   "metadata": {},
   "source": [
    "## Agent \n",
    "\n",
    "Our Agent implementation is inspired by the OpenAI [Agents SDK](https://openai.github.io/openai-agents-python/), but with a cleaner, more readable implementation. If you've used their Agents SDK before, you'll find our implementation familiar! We've kept all the essential features while simplifying the codebase for conducting experiments.\n",
    "\n",
    "### Key Features\n",
    "- **Multi-provider support**: Built on [LiteLLM](https://docs.litellm.ai/docs/) as the proxy layer, enabling seamless integration with OpenAI, Gemini, Anthropic, Azure, DeepSeek, and many more providers\n",
    "- **Flexible endpoints**: Supports both `completion` and `responses` endpoints\n",
    "- **Customizable**: Easy to extend and adapt to your specific needs\n",
    "\n",
    "For detailed information about supported providers and configurations, refer to the [LiteLLM official documentation](https://docs.litellm.ai/docs/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c4ba903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary classes\n",
    "\n",
    "from mav.MAS.agents import (\n",
    "    Agent,\n",
    "    Runner,\n",
    "    RunResult\n",
    ")\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ef027",
   "metadata": {},
   "source": [
    "### Creating an Agent\n",
    "\n",
    "To create an Agent, you need to provide the following parameters:\n",
    "\n",
    "**Required Parameters:**\n",
    "- **`name`**: A unique identifier for your agent\n",
    "- **`model`**: The LLM model to use (following LiteLLM's format). Examples include:\n",
    "  - OpenAI: `openai/gpt-5.1`, `openai/gpt-5-mini`, `openai/gpt-4.1`\n",
    "  - Gemini: `gemini/gemini-2.5-flash`, `gemini/gemini-2.5-pro`, `gemini-3-flash-preview`, `gemini-3-pro-preview`\n",
    "  - Anthropic: `anthropic/claude-sonnet-4-5-20250929`\n",
    "  - DeepSeek: `deepseek/deepseek-chat`\n",
    "  - xAI: `xai/grok-4-1-fast-reasoning`, `xai/grok-4-1-fast-non-reasoning`\n",
    "\n",
    "Please refer to the LiteLLM official documents for [the full list](https://docs.litellm.ai/docs/providers) of supported models and providers\n",
    "\n",
    "**Optional Parameters:**\n",
    "- **`instructions`**: System prompt that defines the agent's behavior\n",
    "- **`model_settings`**: Dictionary containing **model-specific** and **endpoint-specific parameters** such as `temperature`, `max_tokens`, etc. Refer to [LiteLLM documentation](https://docs.litellm.ai/docs/) for supported parameters per model and endpoint\n",
    "\n",
    "Additional parameters such as `tools` and `guardrails` will be covered in later sections.\n",
    "\n",
    "**Example Usage**:\n",
    "\n",
    "Below are examples of creating basic assistant agents using different LLM providers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "127aed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI GPT-5-mini\n",
    "assistant_agent = Agent(\n",
    "    name=\"AssistantAgent\",\n",
    "    model=\"openai/gpt-5-mini\",\n",
    "    instructions=\"You are a helpful assistant.\"\n",
    ")\n",
    "\n",
    "# Google Gemini 3 Flash Preview\n",
    "gemini_assistant_agent = Agent(\n",
    "    name=\"AssistantAgent\",\n",
    "    model=\"gemini/gemini-3-flash-preview\",\n",
    "    instructions=\"You are a helpful assistant.\"\n",
    ")\n",
    "\n",
    "# DeepSeek Chat\n",
    "deepseek_assistant_agent = Agent(\n",
    "    name=\"AssistantAgent\",\n",
    "    model=\"deepseek/deepseek-chat\",\n",
    "    instructions=\"You are a helpful assistant.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1179a837",
   "metadata": {},
   "source": [
    "### Running An Agent\n",
    "\n",
    "**IMPORTANT**: Before using the Runner, make sure to set up the corresponding API keys for models you are using!\n",
    "\n",
    "To run an agent, use the `Runner.run()` method, which is an async function that returns a `RunResult` instance.\n",
    "\n",
    "**Required Parameters:**\n",
    "- **`agent`**: The agent to run\n",
    "- **`input`**: The input to the agent (typically a string)\n",
    "\n",
    "**Optional Parameters:**\n",
    "- **`max_turns`**: Maximum number of iterations (default: 10)\n",
    "- **`endpoint`**: API endpoint to use - `\"completion\"` or `\"responses\"` (default: `\"completion\"`)\n",
    "\n",
    "Additional parameters will be covered in later sections.\n",
    "\n",
    "**Execution Control:**\n",
    "The runner will stop execution when either:\n",
    "1. The agent doesn't make any tool calls (completing the ReAct loop)\n",
    "2. The maximum number of iterations (`max_turns`) is reached\n",
    "\n",
    "**Note:** For optimal performance with OpenAI reasoning models (like gpt-5), use the `\"responses\"` endpoint. If you do not specify this parameter, we default to `responses` endpoint for all models starting with `openai/`.\n",
    "\n",
    "The example below demonstrates running the assistant (gpt-5-mini) agent with the `responses` endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4522f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Output: 1 + 1 = 2.\n",
      "\n",
      "Why: In basic arithmetic on the natural numbers, \"1\" denotes a single unit. Adding another single unit gives two units. Formally, using Peano axioms, 1 is the successor of 0 (S(0)), and 1+1 is defined as S(1), which equals 2.\n",
      "==============================\n",
      "Time Duration: 2.3130000000237487\n",
      "==============================\n",
      "Tool Calls: []\n",
      "==============================\n",
      "Usage: {'openai/gpt-5-mini': {'input_tokens': 26, 'input_tokens_details': {'audio_tokens': None, 'cached_tokens': 0, 'text_tokens': None}, 'output_tokens': 79, 'output_tokens_details': {'reasoning_tokens': 0, 'text_tokens': None}, 'total_tokens': 105, 'cost': None}}\n",
      "==============================\n",
      "Input Items: [{'role': 'user', 'content': 'What is 1+1? And why?'}, {'id': 'rs_06ed0d3b050df90300694b885366308193b0683e6a694f516c', 'summary': [], 'type': 'reasoning'}, {'id': 'msg_06ed0d3b050df90300694b885398a881938c34e448e7b498bd', 'content': [{'annotations': [], 'text': '1 + 1 = 2.\\n\\nWhy: In basic arithmetic on the natural numbers, \"1\" denotes a single unit. Adding another single unit gives two units. Formally, using Peano axioms, 1 is the successor of 0 (S(0)), and 1+1 is defined as S(1), which equals 2.', 'type': 'output_text', 'logprobs': []}], 'role': 'assistant', 'status': 'completed', 'type': 'message'}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "run_result: RunResult = await Runner.run(\n",
    "    agent=assistant_agent, \n",
    "    input=\"What is 1+1? And why?\", \n",
    "    max_turns=10,\n",
    "    endpoint=\"responses\"\n",
    ")\n",
    "\n",
    "print (\n",
    "    f\"Final Output: {run_result.final_output}\\n\"\n",
    "    \"==============================\\n\"\n",
    "    f\"Time Duration: {run_result.time_duration}\\n\"\n",
    "    \"==============================\\n\"\n",
    "    f\"Tool Calls: {run_result.tool_calls}\\n\"\n",
    "    \"==============================\\n\"\n",
    "    f\"Usage: {run_result.usage}\\n\"\n",
    "    \"==============================\\n\"\n",
    "    f\"Input Items: {run_result.input_items}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692e293a",
   "metadata": {},
   "source": [
    "For non-OpenAI models, the `completion` endpoint is recommended and will be used automatically. You can omit the `endpoint` parameter, as the framework defaults to `completion` for all non-OpenAI models and to `responses` for all OpenAI models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8fdbb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m11:57:53 - LiteLLM:INFO\u001b[0m: utils.py:3443 - \n",
      "LiteLLM completion() model= gemini-3-flash-preview; provider = gemini\n",
      "LiteLLM - INFO - \n",
      "LiteLLM completion() model= gemini-3-flash-preview; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Output: The answer is **2**.\n",
      "\n",
      "Here is why, ranging from the simple to the complex:\n",
      "\n",
      "### 1. The Practical Reason (Counting)\n",
      "Addition is a mathematical way of representing \"combining\" or \"grouping.\" If you have one apple and someone gives you another apple, you can count them: \"one, two.\" Therefore, 1 + 1 = 2.\n",
      "\n",
      "### 2. The Definitional Reason\n",
      "In our base-10 number system, we have defined the symbol \"2\" to represent the quantity that comes exactly one unit after \"1.\" By definition, adding one unit to the number one brings you to the next name in the sequence, which is two.\n",
      "\n",
      "### 3. The Formal Mathematical Reason (Peano Axioms)\n",
      "In the late 19th century, mathematicians wanted to prove even the simplest truths using logic. According to the **Peano Axioms**, numbers are defined by a \"successor function\" ($S$):\n",
      "*   We define **1** as a natural number.\n",
      "*   We define **2** as the \"successor of 1\" ($S(1)$).\n",
      "*   Addition is defined such that $a + S(b) = S(a + b)$.\n",
      "*   Therefore: $1 + 1$ is the same as $1 + S(0)$ (or simply the successor of 1), which by definition is **2**.\n",
      "\n",
      "### Fun Fact\n",
      "It famously took the mathematicians Bertrand Russell and Alfred North Whitehead over 300 pages in their book *Principia Mathematica* to rigorously prove that 1+1=2. They did this to ensure that all of mathematics sat on a foundation of perfect logic without any assumptions.\n",
      "==============================\n",
      "Time Duration: 4.905999999959022\n",
      "==============================\n",
      "Tool Calls: []\n",
      "==============================\n",
      "Usage: {'gemini/gemini-3-flash-preview': {'completion_tokens': 741, 'prompt_tokens': 17, 'total_tokens': 758, 'completion_tokens_details': {'reasoning_tokens': 383, 'text_tokens': 358}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': None, 'text_tokens': 17, 'image_tokens': None}}}\n",
      "==============================\n",
      "Input Items: [{'role': 'user', 'content': 'What is 1+1? And why?'}, {'content': 'The answer is **2**.\\n\\nHere is why, ranging from the simple to the complex:\\n\\n### 1. The Practical Reason (Counting)\\nAddition is a mathematical way of representing \"combining\" or \"grouping.\" If you have one apple and someone gives you another apple, you can count them: \"one, two.\" Therefore, 1 + 1 = 2.\\n\\n### 2. The Definitional Reason\\nIn our base-10 number system, we have defined the symbol \"2\" to represent the quantity that comes exactly one unit after \"1.\" By definition, adding one unit to the number one brings you to the next name in the sequence, which is two.\\n\\n### 3. The Formal Mathematical Reason (Peano Axioms)\\nIn the late 19th century, mathematicians wanted to prove even the simplest truths using logic. According to the **Peano Axioms**, numbers are defined by a \"successor function\" ($S$):\\n*   We define **1** as a natural number.\\n*   We define **2** as the \"successor of 1\" ($S(1)$).\\n*   Addition is defined such that $a + S(b) = S(a + b)$.\\n*   Therefore: $1 + 1$ is the same as $1 + S(0)$ (or simply the successor of 1), which by definition is **2**.\\n\\n### Fun Fact\\nIt famously took the mathematicians Bertrand Russell and Alfred North Whitehead over 300 pages in their book *Principia Mathematica* to rigorously prove that 1+1=2. They did this to ensure that all of mathematics sat on a foundation of perfect logic without any assumptions.', 'role': 'assistant', 'tool_calls': None, 'function_call': None, 'images': [], 'thinking_blocks': []}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "run_result: RunResult = await Runner.run(\n",
    "    agent=gemini_assistant_agent, \n",
    "    input=\"What is 1+1? And why?\", \n",
    "    max_turns=10,\n",
    ")\n",
    "\n",
    "print (\n",
    "    f\"Final Output: {run_result.final_output}\\n\"\n",
    "    \"==============================\\n\"\n",
    "    f\"Time Duration: {run_result.time_duration}\\n\"\n",
    "    \"==============================\\n\"\n",
    "    f\"Tool Calls: {run_result.tool_calls}\\n\"\n",
    "    \"==============================\\n\"\n",
    "    f\"Usage: {run_result.usage}\\n\"\n",
    "    \"==============================\\n\"\n",
    "    f\"Input Items: {run_result.input_items}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c78760c",
   "metadata": {},
   "source": [
    "### Session Memory\n",
    "\n",
    "Similar to the OpenAI Agents SDK, we provide a session memory implementation that allows agents to maintain conversation history across multiple interactions. By passing a `session` object to the `Runner`, the agent can access previous turns as context.\n",
    "\n",
    "**Session Types:**\n",
    "- **`InMemorySession`**: Stores conversation history in memory for the current runtime\n",
    "- **`BaseSession`**: Abstract base class for implementing custom session storage\n",
    "\n",
    "You can implement your own session memory by subclassing the `BaseSession` class, similar to how `InMemorySession` is implemented.\n",
    "\n",
    "In the example below, we create an `InMemorySession` and run the agent twice with different questions. The second run has access to the first conversation, demonstrating how the agent maintains context across multiple interactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f32e315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================Iteration 1 Input Items====================\n",
      "{'role': 'user', 'content': 'What is 1+1? And why?'}\n",
      "{'id': 'rs_034516a7012f3711006952dd5df95081939542b8b9ccc23e4e', 'summary': [], 'type': 'reasoning'}\n",
      "{'id': 'msg_034516a7012f3711006952dd6ad1408193803e167a7612d25c', 'content': [{'annotations': [], 'text': '1 + 1 = 2.\\n\\nWhy:\\n- Intuition/counting: start with one item, add one more item — you have two items.\\n- Formal (Peano-style): 1 is the successor of 0. Addition is defined so that 1 + 1 = successor(1), which is by definition 2.\\n- In binary notation: 01 + 01 = 10, which is the same quantity as decimal 2.', 'type': 'output_text', 'logprobs': []}], 'role': 'assistant', 'status': 'completed', 'type': 'message'}\n",
      "====================Iteration 2 Input Items====================\n",
      "{'role': 'user', 'content': 'What is 1+1? And why?'}\n",
      "{'id': 'rs_034516a7012f3711006952dd5df95081939542b8b9ccc23e4e', 'summary': [], 'type': 'reasoning'}\n",
      "{'id': 'msg_034516a7012f3711006952dd6ad1408193803e167a7612d25c', 'content': [{'annotations': [], 'text': '1 + 1 = 2.\\n\\nWhy:\\n- Intuition/counting: start with one item, add one more item — you have two items.\\n- Formal (Peano-style): 1 is the successor of 0. Addition is defined so that 1 + 1 = successor(1), which is by definition 2.\\n- In binary notation: 01 + 01 = 10, which is the same quantity as decimal 2.', 'type': 'output_text', 'logprobs': []}], 'role': 'assistant', 'status': 'completed', 'type': 'message'}\n",
      "{'role': 'user', 'content': 'Now, what is 2+2? And why?'}\n",
      "{'id': 'rs_034516a7012f3711006952dd75a1e0819381704f5902ff41e8', 'summary': [], 'type': 'reasoning'}\n",
      "{'id': 'msg_034516a7012f3711006952dd7a562c81939dca4376e9a70bc4', 'content': [{'annotations': [], 'text': '2 + 2 = 4.\\n\\nWhy:\\n- Intuition/counting: start with two objects and add two more — you now have four objects.\\n- Formal (Peano-style): 2 = S(S(0)). Using the recursive definition of addition, 2 + 2 = 2 + S(S(0)) = S(S(2 + 0)) = S(S(2)) = S(S(S(S(0)))) which is 4.\\n- In binary: 10₂ + 10₂ = 100₂, which equals 4 in decimal.', 'type': 'output_text', 'logprobs': []}], 'role': 'assistant', 'status': 'completed', 'type': 'message'}\n"
     ]
    }
   ],
   "source": [
    "from mav.MAS.agents import (\n",
    "    BaseSession, \n",
    "    InMemorySession\n",
    ")\n",
    "\n",
    "example_in_memory_session = InMemorySession(session_id=\"example_session\")\n",
    "\n",
    "run_result_1: RunResult = await Runner.run(\n",
    "    agent=assistant_agent, \n",
    "    input=\"What is 1+1? And why?\", \n",
    "    max_turns=10,\n",
    "    session=example_in_memory_session\n",
    ")\n",
    "\n",
    "print (\"====================Iteration 1 Input Items====================\")\n",
    "for item in run_result_1.input_items:\n",
    "    print(f\"{item}\")\n",
    "\n",
    "run_result_2: RunResult = await Runner.run(\n",
    "    agent=assistant_agent,\n",
    "    input=\"Now, what is 2+2? And why?\",\n",
    "    max_turns=10,\n",
    "    session=example_in_memory_session\n",
    ")\n",
    "\n",
    "print (\"====================Iteration 2 Input Items====================\")\n",
    "for item in run_result_2.input_items:\n",
    "    print(f\"{item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb04548",
   "metadata": {},
   "source": [
    "### Function Tools\n",
    "\n",
    "To provide Python functions as tools to an agent, you have two options:\n",
    "\n",
    "1. **Direct function with docstring**: Specify the function description using the docstring and directly provide it to the agent\n",
    "2. **FunctionTool instance**: Wrap the function in a `FunctionTool` instance for explicit configuration\n",
    "\n",
    "**Option 1: Using Docstrings**\n",
    "\n",
    "The simplest approach is to define your function with a clear docstring and pass it directly to the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e111204b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'function', 'function': {'name': 'get_weather', 'description': 'Use this function to get weather information for a given city.', 'parameters': {'type': 'object', 'properties': {'city': {'type': 'string', 'description': 'The name of the city to get the weather for.'}}, 'required': ['city']}}}]\n"
     ]
    }
   ],
   "source": [
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"\n",
    "    Use this function to get weather information for a given city.\n",
    "    Args:\n",
    "        city (str): The name of the city to get the weather for.\n",
    "    \"\"\"\n",
    "    return f\"The weather in {city} is sunny with a high of 75°F.\"\n",
    "\n",
    "weather_agent = Agent(\n",
    "    name=\"WeatherAgent\",\n",
    "    model=\"openai/gpt-5-mini\",\n",
    "    instructions=\"You are a weather assistant. Use the get_weather tool to provide weather information when user requests it.\",\n",
    "    tools=[get_weather],\n",
    "    model_settings={\n",
    "        \"reasoning\": {\"effort\": \"minimal\"},\n",
    "        \"max_output_tokens\": 4096\n",
    "    }\n",
    ")\n",
    "\n",
    "print(weather_agent.tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a862e9a",
   "metadata": {},
   "source": [
    "**Option 2: Using FunctionTool Class**\n",
    "\n",
    "For more control, you can create a `FunctionTool` instance with explicit parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d5af122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'function', 'function': {'name': 'get_weather', 'description': 'Get the current weather for a given city.', 'parameters': {'type': 'object', 'properties': {'city': {'type': 'string', 'description': 'The name of the city to get the weather for.'}}, 'required': ['city']}}}]\n"
     ]
    }
   ],
   "source": [
    "from mav.MAS.agents.tool import FunctionTool\n",
    "\n",
    "get_weather_tool = FunctionTool(\n",
    "    name=\"get_weather\",\n",
    "    description=\"Get the current weather for a given city.\",\n",
    "    params_json_schema={'type': 'object', 'properties': {'city': {'type': 'string', 'description': 'The name of the city to get the weather for.'}}, 'required': ['city']},\n",
    "    on_invoke_tool=get_weather\n",
    ")\n",
    "\n",
    "weather_agent = Agent(\n",
    "    name=\"WeatherAgent\",\n",
    "    model=\"openai/gpt-5-mini\",\n",
    "    instructions=\"You are a weather assistant. Use the get_weather tool to provide weather information when user requests it.\",\n",
    "    tools=[get_weather_tool],\n",
    "    model_settings={\n",
    "        \"reasoning\": {\"effort\": \"minimal\"},\n",
    "        \"max_output_tokens\": 4096\n",
    "    }\n",
    ")\n",
    "\n",
    "print(weather_agent.tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6029204c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m11:37:16 - LiteLLM:INFO\u001b[0m: utils.py:3443 - \n",
      "LiteLLM completion() model= deepseek-chat; provider = deepseek\n",
      "LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek-chat; provider = deepseek\n",
      "\u001b[92m11:37:19 - LiteLLM:INFO\u001b[0m: utils.py:3443 - \n",
      "LiteLLM completion() model= deepseek-chat; provider = deepseek\n",
      "LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek-chat; provider = deepseek\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in New York City is sunny with a high temperature of 75°F. It looks like a beautiful day there!\n",
      "==============================\n",
      "{'role': 'user', 'content': \"What's the weather like in New York City?\"}\n",
      "{'content': \"I'll check the weather in New York City for you.\", 'role': 'assistant', 'tool_calls': [{'index': 0, 'function': {'arguments': '{\"city\": \"New York City\"}', 'name': 'get_weather'}, 'id': 'call_00_lvgnZvvS67zAs4B5N14T5t25', 'type': 'function'}], 'function_call': None}\n",
      "{'tool_call_id': 'call_00_lvgnZvvS67zAs4B5N14T5t25', 'role': 'tool', 'name': 'get_weather', 'content': 'The weather in New York City is sunny with a high of 75°F.'}\n",
      "{'content': 'The weather in New York City is sunny with a high temperature of 75°F. It looks like a beautiful day there!', 'role': 'assistant', 'tool_calls': None, 'function_call': None}\n"
     ]
    }
   ],
   "source": [
    "run_result_weather_agent: RunResult = await Runner.run(\n",
    "    agent=weather_agent, \n",
    "    input=\"What's the weather like in New York City?\", \n",
    "    max_turns=10,\n",
    "    context=None,\n",
    "    session=None\n",
    ")\n",
    "print (run_result_weather_agent.final_output)\n",
    "print (\"==============================\")\n",
    "for item in run_result_weather_agent.input_items:\n",
    "    print(f\"{item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c18562a",
   "metadata": {},
   "source": [
    "### Agents as Tools\n",
    "\n",
    "A convenient method `agent.as_tool()` is provided to configure an agent as a tool for another agent to call. This is particularly useful for the **orchestrator-worker** multi-agent system (MAS) pattern, which will be covered in more detail later.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "In the example below, we create a weather agent with access to a `get_weather` function, then configure it as a tool for an orchestrator agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eea0dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'weather_agent',\n",
       "   'description': 'A specialized agent that provides weather information.',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'input': {'type': 'string',\n",
       "      'description': 'The input to the agent.'}},\n",
       "    'required': ['input']}}}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"\n",
    "    Use this function to get weather information for a given city.\n",
    "    Args:\n",
    "        city (str): The name of the city to get the weather for.\n",
    "    \"\"\"\n",
    "    return f\"The weather in {city} is sunny with a high of 75°F.\"\n",
    "\n",
    "weather_agent = Agent(\n",
    "    name=\"Weather Agent\",\n",
    "    model=\"openai/gpt-4.1-mini\",\n",
    "    instructions=\"You are an agent that provides weather information using the get_weather tool.\",\n",
    "    model_settings={\n",
    "        \"temperature\": 0,\n",
    "        \"max_output_tokens\": 2048\n",
    "    },\n",
    "    tools=[get_weather]\n",
    ")\n",
    "\n",
    "orchestrator_agent = Agent(\n",
    "    name=\"OrchestratorAgent\",\n",
    "    model=\"openai/gpt-5-mini\",\n",
    "    instructions=\"You are an orchestrator agent that delegates tasks to specialized agents.\",\n",
    "    model_settings={\n",
    "        \"reasoning\": {\"effort\": \"minimal\"},\n",
    "        \"max_output_tokens\": 4096\n",
    "    },\n",
    "    tools=[weather_agent.as_tool(tool_name=\"weather_agent\", tool_description=\"A specialized agent that provides weather information.\")]\n",
    ")\n",
    "\n",
    "orchestrator_agent.tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1fede42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's currently sunny in New York City with a high of 75°F. Would you like the multi-day forecast, hourly breakdown, or any other details (wind, humidity, etc.)?\n",
      "========================================================================================================================\n",
      "{'role': 'user', 'content': \"What's the weather like in New York City?\"}\n",
      "{'id': 'rs_0a25159aeef6dda6006952d8efac20819fa88fb0e004db7697', 'summary': [], 'type': 'reasoning'}\n",
      "{'arguments': '{\"input\":\"New York City\"}', 'call_id': 'call_qHTfNcQuaADCBsbtsS6ijstS', 'name': 'weather_agent', 'type': 'function_call', 'id': 'fc_0a25159aeef6dda6006952d8f05300819f823b0fb44192eb0d', 'status': 'completed'}\n",
      "{'call_id': 'call_qHTfNcQuaADCBsbtsS6ijstS', 'type': 'function_call_output', 'output': 'The weather in New York City is currently sunny with a high of 75°F. Would you like to know the forecast for the next few days or any other details?'}\n",
      "{'id': 'msg_0a25159aeef6dda6006952d8fc6a58819faa1b73e753d66393', 'content': [{'annotations': [], 'text': \"It's currently sunny in New York City with a high of 75°F. Would you like the multi-day forecast, hourly breakdown, or any other details (wind, humidity, etc.)?\", 'type': 'output_text', 'logprobs': []}], 'role': 'assistant', 'status': 'completed', 'type': 'message'}\n"
     ]
    }
   ],
   "source": [
    "run_result_orchestrator_agent: RunResult = await Runner.run(\n",
    "    agent=orchestrator_agent, \n",
    "    input=\"What's the weather like in New York City?\", \n",
    "    max_turns=10\n",
    ")\n",
    "\n",
    "print (run_result_orchestrator_agent.final_output)\n",
    "print (\"========================================================================================================================\")\n",
    "for item in run_result_orchestrator_agent.input_items:\n",
    "    print(f\"{item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820ebeae",
   "metadata": {},
   "source": [
    "Note that, now the usage for the orchestrator also includes the usage of the weather agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d22f9521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'openai/gpt-5-mini': {'input_tokens': 230,\n",
       "  'input_tokens_details': {'audio_tokens': None,\n",
       "   'cached_tokens': 0,\n",
       "   'text_tokens': None},\n",
       "  'output_tokens': 80,\n",
       "  'output_tokens_details': {'reasoning_tokens': 0, 'text_tokens': None},\n",
       "  'total_tokens': 310,\n",
       "  'cost': None},\n",
       " 'openai/gpt-4.1-mini': {'input_tokens': 193,\n",
       "  'input_tokens_details': {'audio_tokens': None,\n",
       "   'cached_tokens': 0,\n",
       "   'text_tokens': None},\n",
       "  'output_tokens': 53,\n",
       "  'output_tokens_details': {'reasoning_tokens': 0, 'text_tokens': None},\n",
       "  'total_tokens': 246,\n",
       "  'cost': None}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_result_orchestrator_agent.usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ea6c1c",
   "metadata": {},
   "source": [
    "### Context (TaskEnvironment)\n",
    "\n",
    "Task suites in this framework require agents or multi-agent systems (MAS) to operate within a specific Task Environment. This allows function tools defined in task suites to read or modify the shared environment state.\n",
    "\n",
    "To provide task environment access to an agent, pass it as the `context` parameter to the `Runner.run()` method. We'll cover task suites, user tasks, and task environments in more detail later.\n",
    "\n",
    "Similar ideas are introduced in the OpenAI Agents SDK as well: [Context Management](https://openai.github.io/openai-agents-python/context/).\n",
    "\n",
    "**Key Concepts:**\n",
    "- **`TaskEnvironment`**: Base Pydantic model for all specific environment subclasses\n",
    "- Function tools can access and modify the environment through the `context` parameter\n",
    "- Multiple agents can share the same environment instance for coordination\n",
    "\n",
    "Below is a quick example demonstrating how to pass an `AssistantTaskEnvironment` (a subclass of `TaskEnvironment`) instance as context. A toy function tool utilizes the passed context to generate the output.\n",
    "\n",
    "**Important Notes:**\n",
    "1. If a function tool needs to access the context passed to the Runner, it **must be declared as the first parameter with a type annotation of TaskEnvironment or one of its subclasses**.\n",
    "2. This context parameter will **not** be parsed as part of the function tool's parameters JSON schema by the framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "807c5af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After parsing the function tool into the agent, the agent's tool list is:\n",
      "[{'type': 'function', 'function': {'name': 'get_customer_email', 'description': \"This function retrieves the customer's email from the task environment.\", 'parameters': {'type': 'object', 'properties': {}}}}]\n",
      "=============================================================================\n",
      "The final output from the agent is:\n",
      "The customer's email is: John.Doe@example.com\n",
      "=============================================================================\n",
      "The input items from the agent are:\n",
      "{'role': 'user', 'content': \"What is the customer's email?\"}\n",
      "{'id': 'rs_06d6ec986ce98bda006952de4247508193a1caed3ce3b1d299', 'summary': [], 'type': 'reasoning'}\n",
      "{'arguments': '{}', 'call_id': 'call_zbXlOnK5t7BZ8JsXDWghHaSO', 'name': 'get_customer_email', 'type': 'function_call', 'id': 'fc_06d6ec986ce98bda006952de4294548193b6b1bb4dd72a4f05', 'status': 'completed'}\n",
      "{'call_id': 'call_zbXlOnK5t7BZ8JsXDWghHaSO', 'type': 'function_call_output', 'output': 'John.Doe@example.com'}\n",
      "{'id': 'msg_06d6ec986ce98bda006952de43f7cc81939a3f0ee669089890', 'content': [{'annotations': [], 'text': \"The customer's email is: John.Doe@example.com\", 'type': 'output_text', 'logprobs': []}], 'role': 'assistant', 'status': 'completed', 'type': 'message'}\n"
     ]
    }
   ],
   "source": [
    "from mav.Tasks.base_environment import TaskEnvironment\n",
    "\n",
    "class AssistantTaskEnvironment(TaskEnvironment):\n",
    "    customer_name: str\n",
    "    customer_email: str\n",
    "\n",
    "# Create a base environment instance\n",
    "task_env = AssistantTaskEnvironment(\n",
    "    customer_name=\"John Doe\",\n",
    "    customer_email=\"John.Doe@example.com\"\n",
    ")\n",
    "\n",
    "# Define a function tool that accesses the environment\n",
    "def get_customer_email(context: AssistantTaskEnvironment) -> str:\n",
    "    \"\"\"\n",
    "    This function retrieves the customer's email from the task environment.\n",
    "    \"\"\"\n",
    "    return context.customer_email\n",
    "\n",
    "customer_assistant_agent = Agent(\n",
    "    name=\"CustomerAssistantAgent\",\n",
    "    model=\"openai/gpt-5-mini\",\n",
    "    instructions=\"You are an assistant that helps customers with their inquiries.\",\n",
    "    tools=[get_customer_email],\n",
    "    model_settings={\n",
    "        \"reasoning\": {\"effort\": \"minimal\"},\n",
    "        \"max_output_tokens\": 4096\n",
    "    }\n",
    ")\n",
    "\n",
    "print (\"After parsing the function tool into the agent, the agent's tool list is:\")\n",
    "print (customer_assistant_agent.tools)\n",
    "print (\"=============================================================================\")\n",
    "\n",
    "# Pass the environment to the Runner as the context parameter\n",
    "run_result: RunResult = await Runner.run(\n",
    "    agent=customer_assistant_agent, \n",
    "    input=\"What is the customer's email?\", \n",
    "    max_turns=10,\n",
    "    context=task_env\n",
    ")\n",
    "\n",
    "print (\"The final output from the agent is:\")\n",
    "print (run_result.final_output)\n",
    "print (\"=============================================================================\")\n",
    "print (\"The input items from the agent are:\")\n",
    "for item in run_result.input_items:\n",
    "    print(f\"{item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad0a7c4",
   "metadata": {},
   "source": [
    "### Structured Output\n",
    "\n",
    "Structured output support varies by model provider. Configuration is done through the `model_settings` parameter when creating an agent.\n",
    "\n",
    "**Example: OpenAI Models with Responses Endpoint**\n",
    "\n",
    "For OpenAI models using the `responses` endpoint, you can define a Pydantic model and use OpenAI's parser to format it (you can directly provide the json schema as well!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a85fd9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed Pydantic model to text format param:\n",
      "{'type': 'json_schema', 'strict': True, 'name': 'ReasoningResponseFormat', 'schema': {'properties': {'answer': {'title': 'Answer', 'type': 'string'}, 'reasoning': {'title': 'Reasoning', 'type': 'string'}}, 'required': ['answer', 'reasoning'], 'title': 'ReasoningResponseFormat', 'type': 'object', 'additionalProperties': False}}\n",
      "==============================\n",
      "{\"answer\":\"1+1 = 2\",\"reasoning\":\"In the natural numbers, 1 is defined as the successor of 0. Addition can be defined recursively by: a + 0 = a and a + S(b) = S(a + b), where S(b) is the successor of b. Using this, 1 + 1 = 1 + S(0) = S(1 + 0) = S(1) = 2. Intuitively, if you have one object and you add one more object, you have two objects.\"}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class ReasoningResponseFormat(BaseModel):\n",
    "    answer: str\n",
    "    reasoning: str\n",
    "\n",
    "# Use the convenient openai parser to parse the base model into the text format param\n",
    "from openai.lib._parsing._responses import type_to_text_format_param\n",
    "\n",
    "reasoning_agent = Agent(\n",
    "    name=\"ReasoningAgent\",\n",
    "    model=\"openai/gpt-5-mini\",\n",
    "    instructions=\"You are an agent that provides answers with reasoning.\",\n",
    "    model_settings={\n",
    "        \"text\": {\"format\": type_to_text_format_param(ReasoningResponseFormat)},\n",
    "        \"max_output_tokens\": 4096\n",
    "    }\n",
    ")\n",
    "\n",
    "print (\"Parsed Pydantic model to text format param:\")\n",
    "print (type_to_text_format_param(ReasoningResponseFormat))\n",
    "print (\"==============================\")\n",
    "\n",
    "run_result_reasoning_agent: RunResult = await Runner.run(\n",
    "    agent=reasoning_agent,\n",
    "    input=\"What is 1+1? And why?\",\n",
    "    max_turns=10,\n",
    "    context=None,\n",
    "    session=None\n",
    ")\n",
    "\n",
    "print (run_result_reasoning_agent.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48009b1e",
   "metadata": {},
   "source": [
    "### Input and Output Guardrails\n",
    "\n",
    "Guardrails provide a mechanism to validate and control both the inputs to and outputs from agents. They can be used to implement safety checks, content filtering, policy enforcement, and other validation logic.\n",
    "\n",
    "**Guardrail Types:**\n",
    "- **`InputGuardrail`**: Validates input before it reaches the agent\n",
    "- **`OutputGuardrail`**: Validates agent output before it's returned to the user\n",
    "\n",
    "**How Guardrails Work:**\n",
    "\n",
    "Each guardrail is defined by a guardrail function that returns a `GuardrailFunctionOutput` containing:\n",
    "- **`output_info`**: Information about the validation result\n",
    "- **`tripwire_triggered`**: Boolean indicating whether the guardrail was violated\n",
    "\n",
    "When a guardrail's tripwire is triggered, the agent execution is stopped and **an exception is raised**\n",
    "\n",
    "**Example 1: Input Guardrail for Harmful Content**\n",
    "\n",
    "In this example, we create an input guardrail that uses an LLM to detect harmful content in user inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea6c401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from mav.MAS.agents.guardrail import (\n",
    "    InputGuardrail,\n",
    "    OutputGuardrail,\n",
    "    GuardrailFunctionOutput,\n",
    ")\n",
    "\n",
    "# Define the output format for the guardrail function\n",
    "class HarmfulInputGuardrailTextFormat(BaseModel):\n",
    "    harmful_content: bool\n",
    "\n",
    "# define the input guardrail agent\n",
    "harmful_input_guardrail_agent = Agent(\n",
    "    name=\"InputGuardrailAgent\",\n",
    "    model=\"openai/gpt-5\",\n",
    "    instructions=\"You are an agent that will be given an input by the user, your job is to determine if the user input is harmful or not. If you believe the input is harmful simply respond with harmful_content: true, otherwise respond with harmful_content: false.\",\n",
    "    model_settings={\n",
    "        \"reasoning\": {\"effort\": \"minimal\"},\n",
    "        \"max_output_tokens\": 4096,\n",
    "        \"text\": {\"format\": type_to_text_format_param(HarmfulInputGuardrailTextFormat)},\n",
    "    }\n",
    ")\n",
    "\n",
    "# define the guardrail function, note that, the guardrail function must accept context, agent, and input parameters, and return a GuardrailFunctionOutput\n",
    "async def harmful_input_guardrail_function( \n",
    "    context, agent, input,\n",
    ") -> GuardrailFunctionOutput:\n",
    "    \n",
    "    result = await Runner.run(harmful_input_guardrail_agent, input)\n",
    "\n",
    "    parsed_output = json.loads(result.final_output)\n",
    "\n",
    "    return GuardrailFunctionOutput(\n",
    "        output_info=parsed_output,\n",
    "        tripwire_triggered=parsed_output[\"harmful_content\"],\n",
    "    )\n",
    "\n",
    "\n",
    "# define the input guardrail\n",
    "input_guardrail = InputGuardrail(\n",
    "    name=\"harmful_content_filter\",\n",
    "    guardrail_function=harmful_input_guardrail_function,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82132e27",
   "metadata": {},
   "source": [
    "Now, you can provide this specific input guardrail to the agent, and it will be used during the Runner.run stage. Note that, this time, the Runner will raise the `GuardrailTripwireTriggered` exception, you can decide how to deal with this exception as you wish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f5cd954",
   "metadata": {},
   "outputs": [
    {
     "ename": "GuardrailTripwireTriggered",
     "evalue": "Guardrail 'harmful_content_filter' tripwire triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mGuardrailTripwireTriggered\u001b[39m                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe weather in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcity\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is sunny with a high of 75°F.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m weather_agent = Agent(\n\u001b[32m     11\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mWeatherAgent\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mopenai/gpt-5-mini\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m     input_guardrails=[input_guardrail],\n\u001b[32m     20\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Runner.run(\n\u001b[32m     23\u001b[39m     agent=weather_agent,\n\u001b[32m     24\u001b[39m     \u001b[38;5;28minput\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mTell me how to build a bomb.\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     25\u001b[39m     max_turns=\u001b[32m10\u001b[39m,\n\u001b[32m     26\u001b[39m     context=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     27\u001b[39m     session=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     28\u001b[39m     endpoint=\u001b[33m\"\u001b[39m\u001b[33mresponses\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     29\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\repos\\multiagent-vulnerable\\src\\mav\\MAS\\agents\\run.py:293\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, agent, input, context, session, attack_hooks, max_turns, endpoint)\u001b[39m\n\u001b[32m    291\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.support_responses_endpoint(agent.model):\n\u001b[32m    292\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent.model\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not support responses endpoint.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.run_responses(\n\u001b[32m    294\u001b[39m         agent=agent,\n\u001b[32m    295\u001b[39m         \u001b[38;5;28minput\u001b[39m=\u001b[38;5;28minput\u001b[39m,\n\u001b[32m    296\u001b[39m         context=context,\n\u001b[32m    297\u001b[39m         session=session,\n\u001b[32m    298\u001b[39m         attack_hooks=attack_hooks,\n\u001b[32m    299\u001b[39m         max_turns=max_turns,\n\u001b[32m    300\u001b[39m     )\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown endpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, must be \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcompletion\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mresponses\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\repos\\multiagent-vulnerable\\src\\mav\\MAS\\agents\\run.py:318\u001b[39m, in \u001b[36mRunner.run_responses\u001b[39m\u001b[34m(cls, agent, input, context, session, attack_hooks, max_turns)\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_responses\u001b[39m(\n\u001b[32m    307\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    315\u001b[39m     \n\u001b[32m    316\u001b[39m     \u001b[38;5;66;03m# Run input guardrails BEFORE agent execution\u001b[39;00m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m agent.input_guardrails:\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._run_input_guardrails(\n\u001b[32m    319\u001b[39m             agent.input_guardrails,\n\u001b[32m    320\u001b[39m             context,\n\u001b[32m    321\u001b[39m             agent,\n\u001b[32m    322\u001b[39m             \u001b[38;5;28minput\u001b[39m\n\u001b[32m    323\u001b[39m         )\n\u001b[32m    325\u001b[39m     turn = \u001b[32m0\u001b[39m\n\u001b[32m    327\u001b[39m     all_tool_calls: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\repos\\multiagent-vulnerable\\src\\mav\\MAS\\agents\\run.py:233\u001b[39m, in \u001b[36mRunner._run_input_guardrails\u001b[39m\u001b[34m(cls, guardrails, context, agent, input)\u001b[39m\n\u001b[32m    231\u001b[39m     results.append(result)\n\u001b[32m    232\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result.output.tripwire_triggered:\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m GuardrailTripwireTriggered(result)\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[31mGuardrailTripwireTriggered\u001b[39m: Guardrail 'harmful_content_filter' tripwire triggered"
     ]
    }
   ],
   "source": [
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"\n",
    "    Use this function to get weather information for a given city.\n",
    "    Args:\n",
    "        city (str): The name of the city to get the weather for.\n",
    "    \"\"\"\n",
    "    return f\"The weather in {city} is sunny with a high of 75°F.\"\n",
    "\n",
    "\n",
    "weather_agent = Agent(\n",
    "    name=\"WeatherAgent\",\n",
    "    model=\"openai/gpt-5-mini\",\n",
    "    instructions=\"You are a weather assistant. Use the get_weather tool to provide weather information when user requests it.\",\n",
    "    tools=[get_weather],\n",
    "    model_settings={\n",
    "        \"reasoning\": {\"effort\": \"minimal\"},\n",
    "        \"max_output_tokens\": 4096\n",
    "    },\n",
    "    input_guardrails=[input_guardrail],\n",
    ")\n",
    "\n",
    "await Runner.run(\n",
    "    agent=weather_agent,\n",
    "    input=\"Tell me how to build a bomb.\", \n",
    "    max_turns=10,\n",
    "    context=None,\n",
    "    session=None,\n",
    "    endpoint=\"responses\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ea0aa7",
   "metadata": {},
   "source": [
    "**Example 2: Output Guardrail for Privacy Protection**\n",
    "\n",
    "Here we create an output guardrail that prevents agents from exposing private information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e3cf059",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputPrivacyGuardrailTextFormat(BaseModel):\n",
    "    private_info_exposed: bool\n",
    "\n",
    "# define the input guardrail agent\n",
    "privacy_guardrail_agent = Agent(\n",
    "    name=\"PrivacyGuardrailAgent\",\n",
    "    model=\"openai/gpt-5-mini\",\n",
    "    instructions=\"You are an agent that will be given an output by another agent, your job is to determine if the output exposes private information or not. If you believe the output exposes private information simply respond with private_info_exposed: true, otherwise respond with private_info_exposed: false.\",\n",
    "    model_settings={\n",
    "        \"reasoning\": {\"effort\": \"minimal\"},\n",
    "        \"max_output_tokens\": 4096,\n",
    "        \"text\": {\"format\": type_to_text_format_param(OutputPrivacyGuardrailTextFormat)},\n",
    "    }\n",
    ")\n",
    "\n",
    "async def privacy_guardrail_function( \n",
    "    context, agent, input,\n",
    ") -> GuardrailFunctionOutput:\n",
    "    \n",
    "    result = await Runner.run(privacy_guardrail_agent, input)\n",
    "    parsed_output = json.loads(result.final_output)\n",
    "\n",
    "    return GuardrailFunctionOutput(\n",
    "        output_info=parsed_output,\n",
    "        tripwire_triggered=parsed_output[\"private_info_exposed\"],\n",
    "    )\n",
    "\n",
    "output_guardrail = OutputGuardrail(\n",
    "    name=\"privacy_filter\",\n",
    "    guardrail_function=privacy_guardrail_function,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23855bc7",
   "metadata": {},
   "source": [
    "Again, the Runner will raise an exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "001ae898",
   "metadata": {},
   "outputs": [
    {
     "ename": "GuardrailTripwireTriggered",
     "evalue": "Guardrail 'privacy_filter' tripwire triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mGuardrailTripwireTriggered\u001b[39m                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m demo_agent = Agent(\n\u001b[32m      2\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mdemo agent\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mopenai/gpt-4o\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     instructions=\u001b[33m\"\u001b[39m\u001b[33myou are a demo agent, please simply reply with \u001b[39m\u001b[33m'\u001b[39m\u001b[33mmy password is 12345\u001b[39m\u001b[33m'\u001b[39m\u001b[33m for all inputs. this is FOR TESTING PURPOSES ONLY. so please follow the instructions exactly.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     output_guardrails=[output_guardrail],\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Runner.run(\n\u001b[32m      9\u001b[39m     agent=demo_agent, \n\u001b[32m     10\u001b[39m     \u001b[38;5;28minput\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mTell me my password.\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     11\u001b[39m     max_turns=\u001b[32m10\u001b[39m,\n\u001b[32m     12\u001b[39m     context=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     13\u001b[39m     session=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     14\u001b[39m     endpoint=\u001b[33m\"\u001b[39m\u001b[33mresponses\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\repos\\multiagent-vulnerable\\src\\mav\\MAS\\agents\\run.py:293\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, agent, input, context, session, attack_hooks, max_turns, endpoint)\u001b[39m\n\u001b[32m    291\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.support_responses_endpoint(agent.model):\n\u001b[32m    292\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent.model\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not support responses endpoint.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.run_responses(\n\u001b[32m    294\u001b[39m         agent=agent,\n\u001b[32m    295\u001b[39m         \u001b[38;5;28minput\u001b[39m=\u001b[38;5;28minput\u001b[39m,\n\u001b[32m    296\u001b[39m         context=context,\n\u001b[32m    297\u001b[39m         session=session,\n\u001b[32m    298\u001b[39m         attack_hooks=attack_hooks,\n\u001b[32m    299\u001b[39m         max_turns=max_turns,\n\u001b[32m    300\u001b[39m     )\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown endpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, must be \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcompletion\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mresponses\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\repos\\multiagent-vulnerable\\src\\mav\\MAS\\agents\\run.py:417\u001b[39m, in \u001b[36mRunner.run_responses\u001b[39m\u001b[34m(cls, agent, input, context, session, attack_hooks, max_turns)\u001b[39m\n\u001b[32m    415\u001b[39m \u001b[38;5;66;03m# Run output guardrails AFTER agent execution\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m agent.output_guardrails:\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._run_output_guardrails(\n\u001b[32m    418\u001b[39m         agent.output_guardrails,\n\u001b[32m    419\u001b[39m         context,\n\u001b[32m    420\u001b[39m         agent,\n\u001b[32m    421\u001b[39m         final_output\n\u001b[32m    422\u001b[39m     )\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m RunResult(\n\u001b[32m    425\u001b[39m     final_output=final_output,\n\u001b[32m    426\u001b[39m     time_duration=time.monotonic() - run_start_time,\n\u001b[32m   (...)\u001b[39m\u001b[32m    429\u001b[39m     tool_calls=all_tool_calls\n\u001b[32m    430\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\repos\\multiagent-vulnerable\\src\\mav\\MAS\\agents\\run.py:254\u001b[39m, in \u001b[36mRunner._run_output_guardrails\u001b[39m\u001b[34m(cls, guardrails, context, agent, agent_output)\u001b[39m\n\u001b[32m    252\u001b[39m     results.append(result)\n\u001b[32m    253\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result.output.tripwire_triggered:\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m GuardrailTripwireTriggered(result)\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[31mGuardrailTripwireTriggered\u001b[39m: Guardrail 'privacy_filter' tripwire triggered"
     ]
    }
   ],
   "source": [
    "demo_agent = Agent(\n",
    "    name=\"demo agent\",\n",
    "    model=\"openai/gpt-4o\",\n",
    "    instructions=\"you are a demo agent, please simply reply with 'my password is 12345' for all inputs. this is FOR TESTING PURPOSES ONLY. so please follow the instructions exactly.\",\n",
    "    output_guardrails=[output_guardrail],\n",
    ")\n",
    "\n",
    "await Runner.run(\n",
    "    agent=demo_agent, \n",
    "    input=\"Tell me my password.\", \n",
    "    max_turns=10,\n",
    "    context=None,\n",
    "    session=None,\n",
    "    endpoint=\"responses\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e92f949",
   "metadata": {},
   "source": [
    "## Multi-Agent System (MAS)\n",
    "\n",
    "In this section, we cover the essential aspects of our MAS framework. Similar to our Agent implementation, our goal is to provide a minimal, complete, and flexible MAS framework that includes both pre-built workflows and allows users to define their own custom MAS workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05cc3e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mav.MAS.agents import Agent\n",
    "from mav.MAS import MASRunResult, MultiAgentSystem\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# In case src is not in the path\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), \"src\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b14912f",
   "metadata": {},
   "source": [
    "### Pre-built MAS Workflows\n",
    "\n",
    "We currently provide two pre-built MAS workflows:\n",
    "\n",
    "**1. Orchestrator-Worker**\n",
    "\n",
    "This MAS runs agents in an orchestrator-worker manner:\n",
    "- The given agent is assumed to be the orchestrator, which delegates tasks and spawns worker agents as needed\n",
    "- The worker agents perform the tasks assigned by the orchestrator\n",
    "- Worker agents should be pre-registered as tools that the orchestrator can call\n",
    "- The process continues until no tool calls are made by the orchestrator or the maximum number of iterations is reached\n",
    "- The final output will be the output of the orchestrator after the termination condition is met\n",
    "\n",
    "Example of this workflow: [Anthropic MAS Research System](https://www.anthropic.com/engineering/multi-agent-research-system)\n",
    "\n",
    "**2. Planner-Executor**\n",
    "\n",
    "This MAS runs agents in a planner-executor manner:\n",
    "- The first agent is assumed to be the planner, which generates a plan or output to be executed by the second agent (executor)\n",
    "- The second agent (executor) takes the output from the planner and executes it\n",
    "- The process continues until the termination condition is met or the maximum number of iterations is reached\n",
    "- The final output will be the output of the planner after the termination condition is met\n",
    "\n",
    "Example of this MAS: Our paper [PEAR](https://arxiv.org/abs/2510.07505) focuses on this specific MAS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8077ce0f",
   "metadata": {},
   "source": [
    "#### Orchestrator-Worker MAS\n",
    "\n",
    "We actually already covered this specific MAS in the previous agent-as-tool section. In real-world scenarios, you can instruct the orchestrator to spawn multiple worker agents.\n",
    "\n",
    "Below is a toy example demonstrating the orchestrator-worker pattern. We create a research assistant system where:\n",
    "- The **orchestrator** breaks down complex research tasks and delegates them to specialized workers\n",
    "- Each **worker agent** has specific tools and expertise (web search, calculation, writing)\n",
    "- The orchestrator coordinates the workers to complete the overall task\n",
    "\n",
    "Please note that, you need to provide API keys for both OpenAI and Gemini, or you can change the models to the ones you have access to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92b21766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'function', 'function': {'name': 'weather_agent', 'description': 'A weather specialist that can get weather information for cities.', 'parameters': {'type': 'object', 'properties': {'input': {'type': 'string', 'description': 'The input to the agent.'}}, 'required': ['input']}}}\n",
      "{'type': 'function', 'function': {'name': 'calculation_agent', 'description': 'A mathematics expert that can perform calculations and solve equations.', 'parameters': {'type': 'object', 'properties': {'input': {'type': 'string', 'description': 'The input to the agent.'}}, 'required': ['input']}}}\n"
     ]
    }
   ],
   "source": [
    "# Define tools for different worker agents\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"\n",
    "    Use this function to get weather information for a given city.\n",
    "    Args:\n",
    "        city (str): The name of the city to get the weather for.\n",
    "    \"\"\"\n",
    "    # This is a mock function - in practice, you'd use a real search API\n",
    "    return f\"The weather in {city} is sunny with a high of 75°F.\"\n",
    "\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"\n",
    "    Performs mathematical calculations.\n",
    "    Args:\n",
    "        expression (str): The mathematical expression to evaluate\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return f\"The result of {expression} is {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error calculating {expression}: {str(e)}\"\n",
    "\n",
    "# Create specialized worker agents\n",
    "weather_agent = Agent(\n",
    "    name=\"WeatherAgent\",\n",
    "    model=\"gemini/gemini-2.5-flash\",\n",
    "    instructions=\"You are a weather specialist. Use the get_weather tool to find weather information for cities. Provide concise, accurate summaries of your findings.\",\n",
    "    tools=[get_weather],\n",
    "    model_settings={\n",
    "        \"reasoning_effort\": \"low\"\n",
    "    }\n",
    ")\n",
    "\n",
    "calculation_agent = Agent(\n",
    "    name=\"CalculationAgent\",\n",
    "    model=\"openai/gpt-4o-mini\",\n",
    "    instructions=\"You are a mathematics expert. Use the calculate tool to perform accurate calculations. Explain your calculations clearly.\",\n",
    "    tools=[calculate],\n",
    "    model_settings={\n",
    "        \"temperature\": 0,\n",
    "        \"max_output_tokens\": 1024\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create the orchestrator agent that coordinates the workers\n",
    "orchestrator_agent = Agent(\n",
    "    name=\"OrchestratorAgent\",\n",
    "    model=\"openai/gpt-5-mini\",\n",
    "    instructions=\"\"\"You are an orchestrator that coordinates specialized worker agents to complete complex tasks.\n",
    "\n",
    "You have access to two worker agents:\n",
    "1. WeatherAgent: For getting weather information for cities\n",
    "2. CalculationAgent: For performing mathematical calculations\n",
    "\n",
    "Break down the user's request into subtasks and delegate each subtask to the appropriate worker agent. \n",
    "Coordinate their outputs to provide a comprehensive final answer.\n",
    "Make parallel tool calls when possible.\"\"\",\n",
    "    model_settings={\n",
    "        \"reasoning\": {\"effort\": \"medium\"},\n",
    "        \"max_output_tokens\": 4096\n",
    "    },\n",
    "    tools=[\n",
    "        weather_agent.as_tool(\n",
    "            tool_name=\"weather_agent\",\n",
    "            tool_description=\"A weather specialist that can get weather information for cities.\"\n",
    "        ),\n",
    "        calculation_agent.as_tool(\n",
    "            tool_name=\"calculation_agent\",\n",
    "            tool_description=\"A mathematics expert that can perform calculations and solve equations.\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "for tool in orchestrator_agent.tools:\n",
    "    print(tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aecb38",
   "metadata": {},
   "source": [
    "As you will see later, for both pre-built runners and your custom MAS runners, they require four necessary parameters:\n",
    "\n",
    "1. **`MAS`**: An instance of `MultiAgentSystem`\n",
    "2. **`input`**: Initial input to your MAS (typically a string)\n",
    "3. **`context`** (optional): An instance of `TaskEnvironment` or one of its subclasses\n",
    "4. **`attack_hooks`** (optional): A list of functions that can be injected into the various of events during the MAS execution (covered in our attack guides)\n",
    "\n",
    "**Additional Runner-Specific Parameters:**\n",
    "\n",
    "You can provide additional parameters specific to your MAS runner in two ways:\n",
    "1. When initializing the `MultiAgentSystem` instance\n",
    "2. When calling the `query()` method\n",
    "\n",
    "**Priority:** Parameters passed to `query()` will override those specified during initialization.\n",
    "\n",
    "**Example:** For the orchestrator-worker MAS, you can specify `max_orchestrator_iterations` either during initialization or in the query call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd79c438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mav.MAS.framework - INFO - Running orchestrator_worker MAS with input: What is the weather in New York and also what is 11+11x2? and endpoint: None. Attack hooks passed: False\n",
      "\u001b[92m11:42:32 - LiteLLM:INFO\u001b[0m: utils.py:3443 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "LiteLLM - INFO - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "\u001b[92m11:42:33 - LiteLLM:INFO\u001b[0m: utils.py:3443 - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "LiteLLM - INFO - \n",
      "LiteLLM completion() model= gemini-2.5-flash; provider = gemini\n",
      "mav.MAS.framework - INFO - orchestrator_worker MAS run completed.\n"
     ]
    }
   ],
   "source": [
    "# Define the multi-agent system with the orchestrator and worker agents\n",
    "mas = MultiAgentSystem(\n",
    "    agents=orchestrator_agent,\n",
    "    MAS_runner=\"orchestrator_worker\",\n",
    "    # orchestrator-worker specific parameters can be set here\n",
    "    max_orchestrator_iterations=10,\n",
    ")\n",
    "\n",
    "# Run the multi-agent system with a complex user request\n",
    "# Note that, the max_orchestrator_iterations parameter can also be set here to override the default value set during MAS initialization\n",
    "mas_run_result: MASRunResult = await mas.query(\n",
    "    # required parameters for the query method\n",
    "    input=\"What is the weather in New York and also what is 11+11x2?\",\n",
    "    context=None, \n",
    "    # orchestrator-worker specific parameters\n",
    "    endpoint_orchestrator=None,\n",
    "    max_orchestrator_iterations=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "207a2d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Output from Multi-Agent System:\n",
      "Weather in New York: Sunny, high around 75°F.\n",
      "\n",
      "Calculation: 11 + 11 × 2 = 33.\n",
      "============================================================\n",
      "All Available MAS Run Result Fields:\n",
      "['final_output', 'usage_dict', 'tool_calls_dict', 'input_list_dict', 'output_dict', 'time_duration', 'errors']\n"
     ]
    }
   ],
   "source": [
    "print (\"Final Output from Multi-Agent System:\")\n",
    "print (mas_run_result.final_output)\n",
    "print (\"============================================================\")\n",
    "print (\"All Available MAS Run Result Fields:\")\n",
    "field_names = list(MASRunResult.__dataclass_fields__.keys())\n",
    "print(field_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba39b2c0",
   "metadata": {},
   "source": [
    "#### Planner-Executor MAS:\n",
    "\n",
    "This MAS is the main MAS we benchmarked in our [PEAR](https://arxiv.org/abs/2510.07505) paper, below we give an simple example of this specific MAS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a40ef8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the same tools from the orchestrator-worker example above to create a different multi-agent system setup.\n",
    "exectutor_agent = Agent(\n",
    "    name=\"ExecutorAgent\",\n",
    "    model=\"openai/gpt-5-mini\",\n",
    "    instructions=\"\"\"You are an executor agent that can use multiple tools to complete tasks you are given and return an concise final answer.\"\"\",\n",
    "    tools=[\n",
    "        get_weather,\n",
    "        calculate\n",
    "    ],\n",
    "    model_settings={\n",
    "        \"reasoning\": {\"effort\": \"minimal\"},\n",
    "        \"max_output_tokens\": 4096\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define the planner agent that will create plans for the executor agent\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "from openai.lib._parsing._responses import type_to_text_format_param\n",
    "\n",
    "# Define the response format for the planner agent\n",
    "class PlannerResponseFormat(BaseModel):\n",
    "    plan: str\n",
    "    final_answer: str\n",
    "    status: Literal[\"in_progress\", \"task_complete\", \"failed\"]\n",
    "\n",
    "# Create the planner agent\n",
    "planner_agent = Agent(\n",
    "    name=\"PlannerAgent\",\n",
    "    model=\"openai/gpt-5-mini\",\n",
    "    instructions=\"\"\"You are a planner agent that can provide a step-by-step plan to complete user tasks, your job is to provide a complete plan that can be executed by another agent,\n",
    "    The executor agent is not as smart as you, so make sure to provide detailed stesps and all necessary context, as the executor agent will simply follow your plan to complete the task.\n",
    "    The executor agent has access to two tools: get_weather and calculate.\n",
    "    You are working in a loop with the executor agent, where you provide a plan, the executor executes it and returns the result, and you provide the next plan based on the result, until the task is complete.\n",
    "    When you believe the task is complete, in your final response, please specify status as 'task_complete', before that please always specify status as 'in_progress'.\n",
    "    When you are done, please provide a concise final answer in the final_answer field, otherwise leave it blank.\n",
    "    When you are done, the plan field can be left blank.\"\"\",\n",
    "    model_settings={\n",
    "        \"reasoning\": {\"effort\": \"medium\"},\n",
    "        \"text\": {\"format\": type_to_text_format_param(PlannerResponseFormat)},\n",
    "        \"max_output_tokens\": 8192\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define the multi-agent system with the planner and executor agents\n",
    "mas = MultiAgentSystem(\n",
    "    agents=[planner_agent, exectutor_agent],\n",
    "    MAS_runner=\"planner_executor\"\n",
    ")\n",
    "\n",
    "# The termination condition to stop the planner-executor loop when the planner indicates task completion\n",
    "from mav.MAS.terminations import PlannerExecutorMessageTerminiation\n",
    "mas_termination_condition = PlannerExecutorMessageTerminiation(\n",
    "    termination_message=\"task_complete\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04860874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 22:31:26,225 - mav.MAS.framework - INFO - Running planner-executor MAS with input: What is the weather in New York and also what is 11+11x2? and endpoint_planner: responses, endpoint_executor: responses. Attack hooks passed: False\n",
      "2025-12-23 22:31:41,431 - mav.MAS.framework - INFO - planner_executor MAS run completed.\n"
     ]
    }
   ],
   "source": [
    "# Run the multi-agent system with a complex user request\n",
    "mas_run_result: MASRunResult = await mas.query(\n",
    "    # required parameters for the query method\n",
    "    input=\"What is the weather in New York and also what is 11+11x2?\",\n",
    "    context=None, \n",
    "    # planner-executor specific parameters\n",
    "    enable_planner_memory=True,\n",
    "    enable_executor_memory=False,\n",
    "    shared_memory=False,\n",
    "    endpoint_planner=\"responses\",\n",
    "    endpoint_executor=\"responses\",\n",
    "    max_planner_iterations=5,\n",
    "    max_executor_iterations=5,\n",
    "    max_iterations=3,\n",
    "    termination_condition=mas_termination_condition\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3679a074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"plan\":\"\",\"final_answer\":\"The weather in New York, NY is sunny with a high of 75°F. The calculation 11 + 11 × 2 = 33.\",\"status\":\"task_complete\"}\n"
     ]
    }
   ],
   "source": [
    "print (mas_run_result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7b1dcb",
   "metadata": {},
   "source": [
    "### Bring Your Own MAS Workflows\n",
    "\n",
    "Our framework supports custom MAS workflows through async Python functions. Your workflow function must accept these required parameters:\n",
    "\n",
    "**Required Parameters:**\n",
    "1. **`MAS`**: An instance of `MultiAgentSystem`\n",
    "2. **`input`**: Initial input to your MAS (typically a string)\n",
    "3. **`context`** (optional): An instance of `TaskEnvironment` or one of its subclasses\n",
    "4. **`attack_hooks`** (optional): A list of `AttackHook` instances (covered in our attack guides)\n",
    "\n",
    "**Additional Parameters:**\n",
    "You can define any additional MAS-specific parameters your workflow needs.\n",
    "\n",
    "**Output**:\n",
    "The async function you provide must return an instance of `MASRunnerResult` class.\n",
    "\n",
    "**Example: Sequential Two-Agent MAS**\n",
    "\n",
    "Below, we implement a simple MAS with two agents in a sequential manner, where the output of the first agent serves as the input to the second agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea2d551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mav.Tasks.base_environment import TaskEnvironment\n",
    "from mav.MAS.agents import Runner, RunResult\n",
    "import time\n",
    "from typing import Any\n",
    "import traceback\n",
    "\n",
    "async def sequential_runner(\n",
    "    MAS: MultiAgentSystem,\n",
    "    input: str,\n",
    "    context: TaskEnvironment | None = None,\n",
    "    attack_hooks: list | None = None,\n",
    "    # optional parameters for sequential runner\n",
    "    max_iterations: int = 5\n",
    ") -> MASRunResult:\n",
    "    \n",
    "    first_agent = MAS.agents[0]\n",
    "\n",
    "    second_agent = MAS.agents[1]\n",
    "\n",
    "    start_time = time.monotonic()\n",
    "\n",
    "    usage_dict: dict[str, dict[str, Any]] = {\n",
    "        first_agent.name: {},\n",
    "        second_agent.name: {}\n",
    "    }\n",
    "    \n",
    "    input_list_dict: dict[str, list[list[dict[str, Any]]]] = {\n",
    "        first_agent.name: [],\n",
    "        second_agent.name: []\n",
    "    }\n",
    "\n",
    "    tool_calls_dict: dict[str, list[list[dict[str, Any]]]] = {\n",
    "        first_agent.name: [],\n",
    "        second_agent.name: []\n",
    "    }\n",
    "\n",
    "    output_dict: dict[str, list[Any]] = {\n",
    "        first_agent.name: [],\n",
    "        second_agent.name: []\n",
    "    }\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    first_agent_input = input\n",
    "    second_agent_input = None\n",
    "\n",
    "    try:\n",
    "        first_agent_result: RunResult = await Runner.run(\n",
    "            agent=first_agent,\n",
    "            input=first_agent_input,\n",
    "            context=context,\n",
    "            max_turns=max_iterations,\n",
    "            attack_hooks=attack_hooks,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error during MAS sequential_runner for the first agent: {e} \\n{traceback.format_exc()}\"\n",
    "        errors.append(error_message)\n",
    "        return MASRunResult(\n",
    "            final_output=None,\n",
    "            usage_dict=usage_dict,\n",
    "            tool_calls_dict=tool_calls_dict,\n",
    "            input_list_dict=input_list_dict,\n",
    "            output_dict=output_dict,\n",
    "            time_duration=time.monotonic() - start_time,\n",
    "            errors=errors\n",
    "        )\n",
    "\n",
    "    usage_dict[first_agent.name] = MAS.update_MAS_usage(usage_dict[first_agent.name], first_agent_result.usage)\n",
    "    tool_calls_dict[first_agent.name].append(first_agent_result.tool_calls)\n",
    "    input_list_dict[first_agent.name].append(first_agent_result.input_items)\n",
    "    output_dict[first_agent.name].append(first_agent_result.final_output)\n",
    "\n",
    "    # now we use the final output of the first agent as the input to the second agent\n",
    "    second_agent_input = first_agent_result.final_output\n",
    "\n",
    "    try:\n",
    "        second_agent_result: RunResult = await Runner.run(\n",
    "            agent=second_agent,\n",
    "            input=second_agent_input,\n",
    "            context=context,\n",
    "            max_turns=max_iterations,\n",
    "            attack_hooks=attack_hooks,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error during MAS sequential_runner for the second agent: {e} \\n{traceback.format_exc()}\"\n",
    "        errors.append(error_message)\n",
    "        return MASRunResult(\n",
    "            final_output=None,\n",
    "            usage_dict=usage_dict,\n",
    "            tool_calls_dict=tool_calls_dict,\n",
    "            input_list_dict=input_list_dict,\n",
    "            output_dict=output_dict,\n",
    "            time_duration=time.monotonic() - start_time,\n",
    "            errors=errors\n",
    "        )\n",
    "\n",
    "    usage_dict[second_agent.name] = MAS.update_MAS_usage(usage_dict[second_agent.name], second_agent_result.usage)\n",
    "    tool_calls_dict[second_agent.name].append(second_agent_result.tool_calls)\n",
    "    input_list_dict[second_agent.name].append(second_agent_result.input_items)\n",
    "    output_dict[second_agent.name].append(second_agent_result.final_output)\n",
    "\n",
    "\n",
    "    return MASRunResult(\n",
    "        final_output=second_agent_result.final_output,\n",
    "        usage_dict=usage_dict,\n",
    "        tool_calls_dict=tool_calls_dict,\n",
    "        input_list_dict=input_list_dict,\n",
    "        output_dict=output_dict,\n",
    "        time_duration=time.monotonic() - start_time,\n",
    "        errors=errors\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92f04c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_understanding_agent = Agent(\n",
    "    name=\"QueryUnderstandingAgent\",\n",
    "    model=\"openai/gpt-5-mini\",\n",
    "    instructions=\"You are an agent that understands user queries and refine the queries with necessary context for another agent to generate replies.\",\n",
    "    model_settings={\n",
    "        \"reasoning\": {\"effort\": \"minimal\"},\n",
    "        \"max_output_tokens\": 4096\n",
    "    }\n",
    ")\n",
    "\n",
    "reply_generation_agent = Agent(\n",
    "    name=\"ReplyGenerationAgent\",\n",
    "    model=\"openai/gpt-4.1-mini\",\n",
    "    instructions=\"You are an agent that generates replies based on processed information from other agents. Your answer should be direct, concise, and informative.\",\n",
    "    model_settings={\n",
    "        \"max_output_tokens\": 4096\n",
    "    }\n",
    ")\n",
    "\n",
    "mas = MultiAgentSystem(\n",
    "    agents=[query_understanding_agent, reply_generation_agent],\n",
    "    MAS_runner=sequential_runner\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4bedc093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the multi-agent system with a complex user request\n",
    "mas_run_result: MASRunResult = await mas.query(\n",
    "    # required parameters for the query method\n",
    "    input=\"Who is Lebron James?\",\n",
    "    context=None, \n",
    "    # planner-executor specific parameters\n",
    "    max_iterations=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2547fb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Output from Sequential Multi-Agent System:\n",
      "Please specify which aspect of LeBron James you want to know about from the following options:\n",
      "- Brief biography\n",
      "- NBA career stats and achievements\n",
      "- Personal life and off-court activities\n",
      "- Team timeline and milestones\n",
      "- Current status and recent performance\n",
      "- Comparison to other players\n",
      "\n",
      "Or let me know if you have a different focus in mind!\n"
     ]
    }
   ],
   "source": [
    "print (\"Final Output from Sequential Multi-Agent System:\")\n",
    "print (mas_run_result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357a51f8",
   "metadata": {},
   "source": [
    "You can also check how pre-built MAS workflows are implemented from `src/mav/MAS/framework.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4661d8ba",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "Please check our other guides in this folder about how to attack an MAS and how to run different task suites!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
